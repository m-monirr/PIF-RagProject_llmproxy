"""
LLM Proxy Manager for RAG Answer Generation
Handles Groq + Ollama Cloud integration with fallback mechanisms
"""

import logging
import openai
from typing import Optional, Dict, List
from pathlib import Path
import subprocess
import time
import requests
import sys
import os

logger = logging.getLogger(__name__)

LLM_PROXY_BASE_URL = "http://localhost:4000"

class LLMProxyManager:
    """Manages LiteLLM proxy for answer generation with fallback support"""
    
    def __init__(self, config_path: str = "llm_proxy_config.yaml", port: int = 4000):
        self.config_path = Path(config_path)
        self.port = port
        self.base_url = f"http://localhost:{port}"  # Use localhost instead of 0.0.0.0
        self.client: Optional[openai.OpenAI] = None
        self.proxy_process = None
        self._is_running = False
        
    def _kill_existing_processes(self):
        """Kill any existing litellm processes"""
        try:
            import psutil
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    cmdline = proc.info.get('cmdline')
                    if cmdline and any('litellm' in str(arg).lower() for arg in cmdline):
                        logger.info(f"Killing existing litellm process (PID: {proc.pid})")
                        proc.kill()
                        proc.wait(timeout=3)
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
        except ImportError:
            logger.warning("psutil not installed, skipping process cleanup")
        except Exception as e:
            logger.warning(f"Error killing existing processes: {e}")
        
        time.sleep(2)
    
    def start_proxy(self) -> bool:
        """Start LiteLLM proxy server"""
        try:
            # Check if already running
            if self._check_proxy_health():
                logger.info(f"‚úÖ LLM proxy already running on port {self.port}")
                self._initialize_client()
                return True
            
            # Check if config file exists
            if not self.config_path.exists():
                logger.error(f"Config file not found: {self.config_path}")
                logger.error(f"Please ensure {self.config_path} exists in the project root")
                return False
            
            # Kill any existing processes
            self._kill_existing_processes()
            
            # Start proxy
            logger.info(f"üöÄ Starting LLM proxy on port {self.port}...")
            logger.info(f"üìã Using config: {self.config_path.absolute()}")
            logger.info(f"üåê Connecting to Groq + Ollama Cloud")
            
            # Use 'litellm' directly instead of 'python -m litellm'
            cmd = [
                "litellm",
                "--port", str(self.port),
                "--config", str(self.config_path.absolute())
            ]
            
            logger.info(f"Running command: {' '.join(cmd)}")
            
            # Start process
            self.proxy_process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1
            )
            
            # Wait for proxy to be ready
            logger.info("‚è≥ Waiting for proxy to start (this may take 20-30 seconds)...")
            max_retries = 20
            for i in range(max_retries):
                # Check if process died
                if self.proxy_process.poll() is not None:
                    stdout, stderr = self.proxy_process.communicate()
                    logger.error(f"‚ùå Proxy process died")
                    logger.error(f"STDOUT:\n{stdout}")
                    logger.error(f"STDERR:\n{stderr}")
                    return False
                
                time.sleep(2)
                
                if self._check_proxy_health():
                    logger.info(f"‚úÖ LLM proxy started successfully!")
                    logger.info(f"   üìç Base URL: {self.base_url}")
                    logger.info(f"   ü§ñ Primary: Groq (llama3-8b)")
                    logger.info(f"   üîÑ Fallbacks: Ollama Cloud models")
                    self._initialize_client()
                    return True
                
                if i % 5 == 0 and i > 0:
                    logger.info(f"   Still waiting... ({i*2}/{max_retries*2}s)")
                
            logger.error("‚ùå Failed to start LLM proxy - timeout after 40 seconds")
            
            # Get error output
            if self.proxy_process:
                try:
                    stdout, stderr = self.proxy_process.communicate(timeout=2)
                    if stdout:
                        logger.error(f"STDOUT:\n{stdout[:1000]}")
                    if stderr:
                        logger.error(f"STDERR:\n{stderr[:1000]}")
                except:
                    pass
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Failed to start LLM proxy: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False
    
    def _check_proxy_health(self, max_retries=2, timeout=5) -> bool:
        """Check if proxy is healthy with retries and shorter timeout"""
        for attempt in range(max_retries):
            try:
                endpoint = f"http://localhost:{self.port}/health"
                
                try:
                    response = requests.get(endpoint, timeout=timeout)
                    if response.status_code == 200:
                        logger.debug(f"‚úÖ Health check passed")
                        return True
                except requests.exceptions.Timeout:
                    logger.debug(f"Health check timeout")
                except requests.exceptions.ConnectionError:
                    logger.debug(f"Connection refused")
                    
            except Exception as e:
                logger.debug(f"Health check error: {e}")
            
            if attempt < max_retries - 1:
                time.sleep(0.5)  # Reduced from 1 second
        
        return False
    
    def _initialize_client(self):
        """Initialize OpenAI client for proxy"""
        try:
            self.client = openai.OpenAI(
                api_key="dummy-key",
                base_url=self.base_url,
                timeout=30.0,  # Reduced from 60
                max_retries=1  # Reduced from 3
            )
            self._is_running = True
            logger.info("‚úÖ OpenAI client initialized for LLM proxy")
        except Exception as e:
            logger.error(f"Failed to initialize client: {e}")
            self._is_running = False
    
    def generate_answer(
        self,
        question: str,
        context: str,
        is_arabic: bool = False,
        chat_history: List[Dict] = None,  # NEW: Add chat history
        max_tokens: int = 500,
        temperature: float = 0.3
    ) -> str:
        """
        Generate answer using LLM with context and chat history
        
        Args:
            question: User's question
            context: Retrieved context from vector DB
            is_arabic: Whether the question is in Arabic
            chat_history: Previous conversation messages
            max_tokens: Maximum tokens in response
            temperature: Sampling temperature
            
        Returns:
            Generated answer string
        """
        if not self._is_running or not self.client:
            logger.warning("LLM proxy not available, using fallback")
            return self._fallback_answer(question, context, is_arabic)
        
        try:
            # Format chat history for prompt
            history_context = ""
            if chat_history and len(chat_history) > 0:
                # Only include last 4 exchanges (8 messages) to avoid token limits
                recent_history = chat_history[-8:] if len(chat_history) > 8 else chat_history
                
                if is_arabic:
                    history_context = "\n\nÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ©:\n"
                    for msg in recent_history:
                        role = "ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ" if msg['role'] == 'user' else "ÿßŸÑŸÖÿ≥ÿßÿπÿØ"
                        history_context += f"{role}: {msg['content']}\n"
                else:
                    history_context = "\n\nPrevious conversation:\n"
                    for msg in recent_history:
                        role = "User" if msg['role'] == 'user' else "Assistant"
                        history_context += f"{role}: {msg['content']}\n"
            
            # Create prompt based on language
            if is_arabic:
                system_prompt = """ÿ£ŸÜÿ™ ŸÖÿ≥ÿßÿπÿØ ÿ∞ŸÉŸä ŸÖÿ™ÿÆÿµÿµ ŸÅŸä ÿ™ÿ≠ŸÑŸäŸÑ ÿ™ŸÇÿßÿ±Ÿäÿ± ÿµŸÜÿØŸàŸÇ ÿßŸÑÿßÿ≥ÿ™ÿ´ŸÖÿßÿ±ÿßÿ™ ÿßŸÑÿπÿßŸÖÿ© ÿßŸÑÿ≥ÿπŸàÿØŸä (PIF).
ŸÖŸáŸÖÿ™ŸÉ ŸáŸä ÿ™ŸÇÿØŸäŸÖ ÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿØŸÇŸäŸÇÿ© ŸàŸÖŸÅÿµŸÑÿ© ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÖŸÇÿØŸÖ ŸÖŸÜ ÿßŸÑÿ™ŸÇÿßÿ±Ÿäÿ± ÿßŸÑÿ≥ŸÜŸàŸäÿ©.

ŸÇŸàÿßÿπÿØ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:
1. ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÖŸÜ ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÖŸÇÿØŸÖ ŸÅŸÇÿ∑
2. ÿ±ÿßÿπŸê ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ© ŸÑŸÅŸáŸÖ ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÉÿßŸÖŸÑ
3. ŸÇÿØŸÖ ÿ•ÿ¨ÿßÿ®ÿßÿ™ Ÿàÿßÿ∂ÿ≠ÿ© ŸàŸÖŸÜÿ∏ŸÖÿ©
4. ÿßÿ∞ŸÉÿ± ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ ŸàÿßŸÑÿ•ÿ≠ÿµÿßÿ¶Ÿäÿßÿ™ ÿπŸÜÿØ ÿ™ŸàŸÅÿ±Ÿáÿß
5. ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ∫Ÿäÿ± ŸÉÿßŸÅŸäÿ©ÿå ÿßÿ∞ŸÉÿ± ÿ∞ŸÑŸÉ ÿ®Ÿàÿ∂Ÿàÿ≠
6. ŸÑÿß ÿ™ÿÆÿ™ŸÑŸÇ ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑÿ≥ŸäÿßŸÇ"""

                user_prompt = f"""ÿßŸÑÿ≥ŸäÿßŸÇ ŸÖŸÜ ÿ™ŸÇÿßÿ±Ÿäÿ± ÿµŸÜÿØŸàŸÇ ÿßŸÑÿßÿ≥ÿ™ÿ´ŸÖÿßÿ±ÿßÿ™ ÿßŸÑÿπÿßŸÖÿ©:
{context}
{history_context}

ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿßŸÑÿ≠ÿßŸÑŸä: {question}

ŸÇÿØŸÖ ÿ•ÿ¨ÿßÿ®ÿ© ÿ¥ÿßŸÖŸÑÿ© ŸàÿØŸÇŸäŸÇÿ© ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑÿ≥ŸäÿßŸÇ ŸàÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ©. ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿ™ŸÜÿ≥ŸäŸÇ Ÿàÿßÿ∂ÿ≠ ŸÖÿπ ŸÜŸÇÿßÿ∑ ŸÖŸÜÿ∏ŸÖÿ© ÿπŸÜÿØ ÿßŸÑÿ∂ÿ±Ÿàÿ±ÿ©."""

            else:
                system_prompt = """You are an intelligent assistant specialized in analyzing Saudi Arabia's Public Investment Fund (PIF) annual reports.
Your task is to provide accurate and detailed answers based on the provided context from annual reports.

Answer Guidelines:
1. Use only information from the provided context
2. Consider previous conversation for full context understanding
3. Provide clear and well-structured answers
4. Include numbers and statistics when available
5. If information is insufficient, state it clearly
6. Do not fabricate information not in the context"""

                user_prompt = f"""Context from PIF Annual Reports:
{context}
{history_context}

Current Question: {question}

Provide a comprehensive and accurate answer based on the context and previous conversation. Use clear formatting with organized bullet points when necessary."""

            # Call LLM through proxy with timeout handling
            try:
                response = self.client.chat.completions.create(
                    model="rag-llm",  # Will use llama-3.1-8b-instant (Groq)
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens,
                    timeout=20.0
                )
                
                answer = response.choices[0].message.content.strip()
                logger.info(f"‚úÖ Generated answer using: {response.model}")
                return answer
                
            except openai.APITimeoutError:
                logger.error("Groq API timeout")
                return self._fallback_answer(question, context, is_arabic)
            except openai.APIConnectionError as e:
                logger.error(f"Groq connection error: {e}")
                return self._fallback_answer(question, context, is_arabic)
            except openai.BadRequestError as e:
                logger.error(f"Groq bad request: {e}")
                return self._fallback_answer(question, context, is_arabic)
                
        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            return self._fallback_answer(question, context, is_arabic)
    
    def _fallback_answer(self, question: str, context: str, is_arabic: bool) -> str:
        """Fallback answer when LLM is unavailable"""
        if is_arabic:
            intro = "ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÖÿ™ÿßÿ≠ÿ© ŸÅŸä ÿ™ŸÇÿßÿ±Ÿäÿ± ÿµŸÜÿØŸàŸÇ ÿßŸÑÿßÿ≥ÿ™ÿ´ŸÖÿßÿ±ÿßÿ™ ÿßŸÑÿπÿßŸÖÿ©:\n\n"
        else:
            intro = "Based on the PIF annual reports:\n\n"
        
        # Simple context-based answer (existing behavior)
        return intro + context[:800] + "..."
    
    def stop_proxy(self):
        """Stop the LLM proxy server"""
        if self.proxy_process:
            try:
                self.proxy_process.terminate()
                self.proxy_process.wait(timeout=5)
                logger.info("‚úÖ LLM proxy stopped")
            except:
                self.proxy_process.kill()
            finally:
                self._is_running = False
                self.client = None
    
    def __enter__(self):
        """Context manager entry"""
        self.start_proxy()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        self.stop_proxy()


# Global proxy instance (singleton pattern)
_proxy_instance: Optional[LLMProxyManager] = None

def get_llm_proxy() -> LLMProxyManager:
    """Get or create global LLM proxy instance with health check"""
    global _proxy_instance
    if _proxy_instance is None:
        _proxy_instance = LLMProxyManager()
        
        # Quick health check (1 retry, 3 second timeout)
        logger.info("üîç Checking for LLM proxy...")
        
        if _proxy_instance._check_proxy_health(max_retries=1, timeout=3):
            _proxy_instance._initialize_client()
            logger.info("‚úÖ Connected to LLM proxy")
        else:
            logger.warning("‚ö†Ô∏è  LLM proxy not available - will use context fallback")
            logger.warning("   Start proxy: python start_llm_proxy_cli.py")
            
    return _proxy_instance
