{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-NdxhbuKugB"
      },
      "outputs": [],
      "source": [
        "!pip install 'litellm'==1.44.9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai-colab')\n",
        "os.environ[\"COHERE_API_KEY\"] = userdata.get('cohere')"
      ],
      "metadata": {
        "id": "Df-SEPDpLprk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion, acompletion\n",
        "from pprint import pprint\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\", # system, assistant\n",
        "        \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = completion(\n",
        "        model=\"cohere/command-r-plus-08-2024\",\n",
        "        messages=messages,\n",
        "        temperature=0.5,\n",
        "        max_tokens=200\n",
        "    )"
      ],
      "metadata": {
        "id": "GZcicgChMEeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jpLFd3b5NB0g",
        "outputId": "3c9b074d-0c57-4ef6-f067-d483373998cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'command-r-plus-08-2024'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "gpXDuBccND7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SDK Logging"
      ],
      "metadata": {
        "id": "qAoLNNX3PZLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import litellm\n",
        "import os\n",
        "import json\n",
        "\n",
        "logs_dir = \"./llm-logs\"\n",
        "os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "def log_success(kwargs, completion_obj, start_time, end_time):\n",
        "    with open(f\"{logs_dir}/success-logs.jsonl\", \"a\") as dest:\n",
        "        dest.write(\n",
        "            json.dumps({\n",
        "                \"kwargs\": kwargs,\n",
        "                \"completion_obj\": completion_obj,\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "            }, ensure_ascii=False, default=str ) + \"\\n\"\n",
        "        )\n",
        "\n",
        "def log_failure(kwargs, completion_obj, start_time, end_time):\n",
        "    with open(f\"{logs_dir}/failure-logs.jsonl\", \"a\") as dest:\n",
        "        dest.write(\n",
        "            json.dumps({\n",
        "                \"kwargs\": kwargs,\n",
        "                \"completion_obj\": completion_obj,\n",
        "                \"start_time\": start_time,\n",
        "                \"end_time\": end_time,\n",
        "            }, ensure_ascii=False, default=str ) + \"\\n\"\n",
        "        )\n",
        "\n",
        "litellm.success_callback = [log_success]\n",
        "litellm.failure_callback = [log_failure]"
      ],
      "metadata": {
        "id": "qpn2fWNBPbmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\", # system, assistant\n",
        "        \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = completion(\n",
        "            model=\"openai/gpt-4o-mini\",\n",
        "            messages=messages,\n",
        "            temperature=0.5,\n",
        "            max_tokens=200\n",
        "        )"
      ],
      "metadata": {
        "id": "KaymhSbuRTOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proxy Server"
      ],
      "metadata": {
        "id": "ZsVOBXD9TE3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'litellm[proxy]'==1.44.9 openai==1.42.0"
      ],
      "metadata": {
        "id": "p7dvX7RoTJV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ check any litellm processes\n",
        "# !pgrep -fl litellm\n",
        "\n",
        "# ============ kill any litellm processes\n",
        "# !pkill -f litellm"
      ],
      "metadata": {
        "id": "_ifl3c6zWrWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai-colab')\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "UCeCZ_CSV-EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llm.yaml\n",
        "model_list:\n",
        "  - model_name: \"groq-gemma9b\"\n",
        "    litellm_params:\n",
        "      model: \"groq/gemma2-9b-it\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "\n",
        "  - model_name: \"groq-mixtral\"\n",
        "    litellm_params:\n",
        "      model: \"groq/mixtral-8x7b-32768\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "\n",
        "  - model_name: \"openai-gpt4o-mini\"\n",
        "    litellm_params:\n",
        "      model: \"openai/gpt-4o-mini\"\n",
        "      api_key: \"os.environ/OPENAI_API_KEY\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUbRqM7iTb_X",
        "outputId": "d85e8fc9-a2c9-4fa0-a2d4-dee72d4207de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting llm.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup litellm --port 4000 --config llm.yaml &\n",
        "!sleep 10 && tail nohup.out"
      ],
      "metadata": {
        "id": "1300kJJYVMxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from pprint import pprint\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    api_key=\"any key\",\n",
        "    base_url=\"http://0.0.0.0:4000\"\n",
        ")"
      ],
      "metadata": {
        "id": "vKMgq4qWW_un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\", # system, assistant\n",
        "        \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"groq-mixtral\",\n",
        "    messages=messages,\n",
        ")"
      ],
      "metadata": {
        "id": "S_p4fQ0pXOyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(\n",
        "    response.choices[0].message.content\n",
        ")"
      ],
      "metadata": {
        "id": "RqSwY4uQXpbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Balancer"
      ],
      "metadata": {
        "id": "h1-qS5cRX8Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llm-lb.yaml\n",
        "model_list:\n",
        "  - model_name: \"global-llm\"\n",
        "    litellm_params:\n",
        "      model: \"groq/gemma2-9b-it\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "      rpm: 20\n",
        "\n",
        "  - model_name: \"global-llm\"\n",
        "    litellm_params:\n",
        "      model: \"groq/mixtral-8x7b-32768\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "      rpm: 20\n",
        "\n",
        "  - model_name: \"global-llm\"\n",
        "    litellm_params:\n",
        "      model: \"openai/gpt-4o-mini\"\n",
        "      api_key: \"os.environ/OPENAI_API_KEY\"\n",
        "      rpm: 10\n",
        "\n",
        "routing_strategy: simple-shuffle # Literal[\"simple-shuffle\", \"least-busy\",]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO_wS7sfX9oK",
        "outputId": "0b39960a-c380-4f52-9431-1c4ac64ccd5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing llm-lb.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup litellm --port 4000 --config llm-lb.yaml &\n",
        "!sleep 10 && tail nohup.out"
      ],
      "metadata": {
        "id": "8eLP5VvDZCmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from pprint import pprint\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    api_key=\"any key\",\n",
        "    base_url=\"http://0.0.0.0:4000\"\n",
        ")"
      ],
      "metadata": {
        "id": "E7NlDZOWZL-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\", # system, assistant\n",
        "        \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"global-llm\",\n",
        "    messages=messages,\n",
        ")"
      ],
      "metadata": {
        "id": "R20zh9MqZO-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jdm-hxVkZY_p",
        "outputId": "d53f7527-e490-4ad8-9dad-2849e50ae365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'groq/mixtral-8x7b-32768'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fallbacks"
      ],
      "metadata": {
        "id": "dgVGKuLcapMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai-colab')\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "7QM1ZVl-ddv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llm-fallback.yaml\n",
        "router_settings:\n",
        "  enable_pre_call_checks: true\n",
        "\n",
        "model_list:\n",
        "  - model_name: \"groq-gemma9b\"\n",
        "    litellm_params:\n",
        "      model: \"groq/gemma2-9b-it\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "\n",
        "  - model_name: \"groq-mixtral\"\n",
        "    litellm_params:\n",
        "      model: \"groq/mixtral-8x7b-32768\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "\n",
        "  - model_name: \"openai-gpt4o-mini\"\n",
        "    litellm_params:\n",
        "      model: \"openai/gpt-4o-mini\"\n",
        "      api_key: \"os.environ/OPENAI_API_KEY\"\n",
        "      rpm: 20\n",
        "\n",
        "litellm_settings:\n",
        "  num_retries: 3\n",
        "  fallbacks: [{\"openai-gpt4o-mini\": \"groq-mixtral\"}]\n",
        "  request_timeout: 10\n",
        "  allowed_fails: 3 # per minute\n",
        "  cooldown_time: 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PpSkqgSaqqM",
        "outputId": "10249fef-0012-4b49-9f31-d974562129fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing llm-fallback.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup litellm --port 4000 --config llm-fallback.yaml &\n",
        "!sleep 10 && tail nohup.out"
      ],
      "metadata": {
        "id": "gtEzu5kudiyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observation"
      ],
      "metadata": {
        "id": "70pO-uGvdz0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai-colab')\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-27f7fa53-b370-46d2-82f0-6f32851dfc92\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-c3571355-5d0c-48bb-ac92-c3dfaecea1c2\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\""
      ],
      "metadata": {
        "id": "MDmKCOl-d2BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "userdata.get('LANGFUSE_SECRET_KEY_X')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pYjqbsc-jN0q",
        "outputId": "0b7d0002-89aa-4cae-93e6-4a7bc3e3caad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pk-lf-27f7fa53-b370-46d2-82f0-6f32851dfc92'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langfuse==2.52.2"
      ],
      "metadata": {
        "id": "PmRvZY1OgDD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llm-lanfuse.yaml\n",
        "model_list:\n",
        "  - model_name: \"groq-gemma9b\"\n",
        "    litellm_params:\n",
        "      model: \"groq/gemma2-9b-it\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "\n",
        "  - model_name: \"groq-mixtral\"\n",
        "    litellm_params:\n",
        "      model: \"groq/mixtral-8x7b-32768\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "\n",
        "  - model_name: \"openai-gpt4o-mini\"\n",
        "    litellm_params:\n",
        "      model: \"openai/gpt-4o-mini\"\n",
        "      api_key: \"os.environ/OPENAI_API_KEY\"\n",
        "\n",
        "litellm_settings:\n",
        "  drop_params: True\n",
        "  success_callback: [\"langfuse\"]\n",
        "  failure_callback: [\"langfuse\"]\n",
        "  redact_user_api_key_info: true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfm6MWZjfQID",
        "outputId": "af9540e5-a060-4e12-ecaa-45955b5ee369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting llm-lanfuse.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup litellm --port 4000 --config llm-lanfuse.yaml &\n",
        "!sleep 10 && tail nohup.out"
      ],
      "metadata": {
        "id": "GiaAQkVggT3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from pprint import pprint\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    api_key=\"any key\",\n",
        "    base_url=\"http://0.0.0.0:4000\"\n",
        ")"
      ],
      "metadata": {
        "id": "vfloPxagilTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\", # system, assistant\n",
        "        \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"openai-gpt4o-mini\",\n",
        "    messages=messages,\n",
        ")"
      ],
      "metadata": {
        "id": "D4c-HqL2ioAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(\n",
        "    response.choices[0].message.content\n",
        ")"
      ],
      "metadata": {
        "id": "K10Sqf87i1TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LiteLLm + LangChain"
      ],
      "metadata": {
        "id": "EiT5j7ZEkST7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai langchain langchain_community"
      ],
      "metadata": {
        "id": "jmhJ-lUFkUf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader([\n",
        "                            \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "                            \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
        "                            \"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\",\n",
        "                        ])\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "5-xe7jaMlzro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_base=\"http://0.0.0.0:4000\",\n",
        "    model = \"openai-gpt4o-mini\",\n",
        "    temperature=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "oMKDiqA4lMkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")]\n",
        ")\n",
        "\n",
        "map_chain = map_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "s31-wQu9nB3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke chain\n",
        "result = map_chain.invoke({\"context\": docs})"
      ],
      "metadata": {
        "id": "igocWVkunKJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "IhdlhARDnOCS",
        "outputId": "f004afc6-d22b-4741-bc13-f8ab4b12e2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The document titled \"LLM Powered Autonomous Agents\" by Lilian Weng discusses the concept of building autonomous agents using large language models (LLMs) as their core controllers. It highlights several proof-of-concept demonstrations, such as AutoGPT, GPT-Engineer, and BabyAGI, showcasing LLMs\\' potential as general problem solvers beyond mere text generation. The document outlines the architecture of LLM-powered agents, which includes components for planning, memory, and tool use. \\n\\nKey components include:\\n1. **Planning**: Agents break down tasks into manageable subgoals and reflect on past actions to improve future performance.\\n2. **Memory**: Agents utilize short-term and long-term memory to retain and recall information, often leveraging external vector stores for efficient retrieval.\\n3. **Tool Use**: Agents can call external APIs to access information and perform tasks beyond their pre-trained capabilities.\\n\\nThe document also addresses challenges faced by LLM-powered agents, such as finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. Overall, it emphasizes the innovative potential of LLMs in creating autonomous agents capable of complex problem-solving and task execution.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}