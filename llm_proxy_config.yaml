# LLM Proxy Configuration for RAG Answer Generation
# Using Ollama Cloud (Free - No local models needed!)

router_settings:
  enable_pre_call_checks: false

model_list:
  # Primary: Ollama Cloud - qwen2.5:3b (Free tier)
  - model_name: "rag-llm"
    litellm_params:
      model: "ollama_chat/qwen2.5:3b"
      api_base: "https://cloud.ollama.ai"
      rpm: 60

  # Fallback: Ollama Cloud - llama3.2:3b
  - model_name: "rag-llm-fallback"
    litellm_params:
      model: "ollama_chat/llama3.2:3b"
      api_base: "https://cloud.ollama.ai"
      rpm: 60

litellm_settings:
  num_retries: 2
  fallbacks: [
    {"rag-llm": ["rag-llm-fallback"]}
  ]
  request_timeout: 60
  drop_params: true
  success_callback: []
  failure_callback: []
