# LLM Proxy Configuration for RAG Answer Generation
router_settings:
  enable_pre_call_checks: true

model_list:
  # Primary: Ollama Cloud (Free)
  - model_name: "rag-llm"
    litellm_params:
      model: "ollama/qwen2.5:3b"
      api_base: "https://cloud.ollama.ai"
      # No API key needed for Ollama Cloud
      rpm: 100  # Adjust based on free tier limits

  # Fallback 1: Ollama Cloud alternative model
  - model_name: "rag-llm-fallback1"
    litellm_params:
      model: "ollama/llama3.2:3b"
      api_base: "https://cloud.ollama.ai"
      rpm: 100

  # Fallback 2: Local Ollama (if available)
  - model_name: "rag-llm-fallback2"
    litellm_params:
      model: "ollama/qwen2.5:3b"
      api_base: "http://localhost:11434"
      rpm: 200

litellm_settings:
  num_retries: 3
  fallbacks: [
    {"rag-llm": ["rag-llm-fallback1", "rag-llm-fallback2"]}
  ]
  request_timeout: 30
  allowed_fails: 5  # per minute
  cooldown_time: 60
  drop_params: true
  success_callback: []
  failure_callback: []
