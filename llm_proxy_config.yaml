# LLM Proxy Configuration for RAG Answer Generation
# Using Ollama Cloud (Free) + Groq (Free - Super Fast!)

router_settings:
  enable_pre_call_checks: false

model_list:
  # Primary: Groq - llama3-8b (FREE, SUPER FAST!)
  - model_name: "rag-llm"
    litellm_params:
      model: "groq/llama3-8b-8192"
      api_key: "os.environ/GROQ_API_KEY"
      rpm: 30

  # Fallback 1: Ollama Cloud - qwen2.5:3b
  - model_name: "rag-llm-ollama"
    litellm_params:
      model: "ollama_chat/qwen2.5:3b"
      api_base: "https://cloud.ollama.ai"
      rpm: 60

  # Fallback 2: Ollama Cloud - llama3.2:3b
  - model_name: "rag-llm-ollama-fallback"
    litellm_params:
      model: "ollama_chat/llama3.2:3b"
      api_base: "https://cloud.ollama.ai"
      rpm: 60

litellm_settings:
  num_retries: 2
  fallbacks: [
    {"rag-llm": ["rag-llm-ollama", "rag-llm-ollama-fallback"]}
  ]
  request_timeout: 60
  drop_params: true
  success_callback: []
  failure_callback: []
