# ğŸ‡¸ğŸ‡¦ PIF RAG Chat - AI-Powered Investment Assistant

An intelligent chatbot for exploring Saudi Arabia's Public Investment Fund (PIF) annual reports using Retrieval-Augmented Generation (RAG).

## ğŸ“ Project Structure

```
ğŸ“ project-v2/API/
â”œâ”€â”€ ğŸ“ config/              # Configuration files
â”‚   â”œâ”€â”€ llm_proxy_config.yaml  # LLM routing config (Groq + Ollama Cloud)
â”‚   â”œâ”€â”€ .env                   # Environment variables (API keys)
â”‚   â””â”€â”€ .env.example           # Template for environment setup
â”‚
â”œâ”€â”€ ğŸ“ data/                # Data storage (auto-generated)
â”‚   â”œâ”€â”€ ğŸ“ pdfs/           # Source PDF files (place your PDFs here)
â”‚   â”œâ”€â”€ ğŸ“ outputs/        # Extraction results (Markdown, tables, images)
â”‚   â”‚   â”œâ”€â”€ output_ar_2021/
â”‚   â”‚   â”œâ”€â”€ output_ar_2022/
â”‚   â”‚   â”œâ”€â”€ output_ar_2023/
â”‚   â”‚   â”œâ”€â”€ output_en_2021/
â”‚   â”‚   â”œâ”€â”€ output_en_2022/
â”‚   â”‚   â””â”€â”€ output_en_2023/
â”‚   â””â”€â”€ ğŸ“ qdrant_storage/ # Vector database persistence
â”‚
â”œâ”€â”€ ğŸ“ src/                 # Core application logic
â”‚   â”œâ”€â”€ ğŸ“ core/           # RAG pipeline components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py      # Configuration constants
â”‚   â”‚   â”œâ”€â”€ extraction.py  # PDF text extraction (Docling)
â”‚   â”‚   â”œâ”€â”€ chunking.py    # Document chunking logic
â”‚   â”‚   â”œâ”€â”€ embedding.py   # Vector embeddings (Ollama qwen3-embedding)
â”‚   â”‚   â””â”€â”€ qdrant_utils.py # Vector DB operations
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ retrieval/      # Query processing & RAG
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rag_query.py   # RAG answer generation with multi-collection search
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ llm/            # LLM integration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ llm_proxy.py   # Multi-provider LLM proxy manager
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ ui/             # User interface (Streamlit)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ components.py  # UI components (sidebar, chat, landing page)
â”‚       â”œâ”€â”€ styles.py      # Custom CSS styling (PIF theme)
â”‚       â””â”€â”€ utils.py       # UI helper functions
â”‚
â”œâ”€â”€ ğŸ“ scripts/             # Utility scripts
â”‚   â”œâ”€â”€ start_llm_proxy.py     # LLM proxy launcher
â”‚   â”œâ”€â”€ run_streamlit.py       # Streamlit launcher
â”‚   â”œâ”€â”€ process_documents.py   # PDF processing pipeline
â”‚   â””â”€â”€ cleanup_old_structure.py # Migration cleanup tool
â”‚
â”œâ”€â”€ ğŸ“ docs/                # Documentation
â”‚   â””â”€â”€ RUN_GUIDE.md       # Detailed setup & troubleshooting guide
â”‚
â”œâ”€â”€ app.py                  # Main Streamlit application entry point
â””â”€â”€ requirements.txt        # Python dependencies
```

## âœ¨ Key Features

### ğŸ“š Document Processing & Knowledge Extraction

- **ğŸ” Advanced PDF Extraction**: Automatically extracts text, tables, and images from PIF annual reports using Docling with OCR support
- **ğŸŒ Bilingual Support**: Seamlessly processes both English and Arabic documents with intelligent language detection
- **ğŸ§© Smart Chunking**: Divides documents into meaningful semantic chunks using HybridChunker with context preservation
- **ğŸ§  High-Quality Embeddings**: Converts text chunks into 4096-dimensional vectors using Ollama's qwen3-embedding model

### ğŸ” Vector Search & Retrieval

- **ğŸ’¡ Semantic Search**: Finds relevant information using vector similarity (cosine distance) in Qdrant
- **ğŸ“Š Multi-Year Search**: Intelligently searches across 2021-2023 reports for comprehensive answers
- **ğŸ“… Year-Specific Filtering**: Automatically prioritizes year-specific information when detected in queries
- **ğŸ¯ Confidence Scoring**: Returns relevance scores and source attribution for transparency

### ğŸ’¬ Chat Interface & User Experience

- **ğŸ¨ Modern Streamlit UI**: Clean, responsive design with Saudi-themed styling (green & gold colors)
- **ğŸ‘¤ Personalized Conversations**: Remembers user name and maintains conversation context
- **â“ Smart Follow-Ups**: Generates contextual follow-up questions based on chat history
- **âŒ¨ï¸ Streaming Responses**: Real-time word-by-word streaming for natural interaction
- **ğŸ“‹ Copy Functionality**: Easy copy-to-clipboard for any message
- **ğŸ‡¸ğŸ‡¦ Full Arabic Support**: Works seamlessly with both English and Arabic queries
- **ğŸ› Debug Mode**: Optional display of sources, confidence scores, and retrieval details

### âœï¸ Answer Generation

- **ğŸ¤– Multi-Provider LLM**: Uses Groq (primary) with automatic fallback to Ollama Cloud
- **ğŸ“ Context-Aware Answers**: Generates comprehensive answers based on retrieved contexts AND chat history
- **ğŸ“‘ Source Attribution**: Transparently cites years and sources for all information
- **ğŸ“Š Well-Formatted Output**: Structured responses with headings, bullet points, and clear organization
- **ğŸ”„ Fallback Mechanism**: Gracefully degrades to context-based answers if LLM is unavailable

## ğŸ› ï¸ Installation & Setup

### Prerequisites

- **Python 3.8+** installed on your system
- **Docker** installed and running (for Qdrant vector database)
- **Ollama** installed locally (for embeddings)
- **Groq API Key** (free tier available at https://console.groq.com/keys)

### Step 1: Clone the repository

```bash
git clone https://github.com/m-monirr/PIF-Annual-Report_RagProject.git
cd PIF-Annual-Report_RagProject/project-v2/API
```

### Step 2: Install dependencies

```bash
pip install -r requirements.txt
```

Key dependencies:
- `streamlit` - Modern web UI framework
- `qdrant-client` - Vector database client
- `ollama` - Local embeddings via qwen3-embedding
- `litellm[proxy]` - Multi-provider LLM routing
- `docling` - PDF extraction and processing
- `langfuse` - LLM observability (optional)

### Step 3: Configure environment

1. **Copy environment template:**
   ```bash
   cp config/.env.example config/.env
   ```

2. **Edit `config/.env` and add your API keys:**
   ```env
   # Groq API Key (FREE - get from https://console.groq.com/keys)
   GROQ_API_KEY=gsk_your_actual_key_here
   
   # Ollama Configuration (local)
   OLLAMA_BASE_URL=http://localhost:11434
   EMBED_MODEL_ID=qwen3-embedding
   
   # Optional: LangFuse for monitoring
   LANGFUSE_PUBLIC_KEY=your_public_key
   LANGFUSE_SECRET_KEY=your_secret_key
   LANGFUSE_HOST=https://cloud.langfuse.com
   ```

### Step 4: Set up services

#### 4.1 Install and Start Ollama (for embeddings)

**Windows:**
```bash
# Download from https://ollama.com/download and install
# Service starts automatically

# Pull the embedding model:
ollama pull qwen3-embedding
```

**Mac/Linux:**
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Start service (separate terminal)
ollama serve

# Pull model
ollama pull qwen3-embedding
```

**Verify:**
```bash
curl http://localhost:11434/api/version
# Should return: {"version":"..."}
```

#### 4.2 Start Qdrant Vector Database

**Windows (Command Prompt):**
```bash
docker run -d -p 6333:6333 -p 6334:6334 -v "%cd%\data\qdrant_storage":/qdrant/storage qdrant/qdrant
```

**Windows (PowerShell):**
```bash
docker run -d -p 6333:6333 -p 6334:6334 -v "${PWD}\data\qdrant_storage":/qdrant/storage qdrant/qdrant
```

**Mac/Linux:**
```bash
docker run -d -p 6333:6333 -p 6334:6334 -v $(pwd)/data/qdrant_storage:/qdrant/storage qdrant/qdrant
```

**Verify:**
```bash
curl http://localhost:6333/collections
# Should return: {"result":{"collections":[]}} (empty on first run)
```

#### 4.3 Start LLM Proxy (for answer generation)

**Terminal 2 (keep this running!):**
```bash
python scripts/start_llm_proxy.py
```

Expected output:
```
ğŸš€ Starting LLM Proxy Server...
âœ… LLM Proxy initialized successfully!
   ğŸ“ Base URL: http://0.0.0.0:4000
   ğŸ¤– Primary: Groq (llama-3.1-8b-instant) - FREE & FAST!
   ğŸ”„ Fallbacks: Ollama Cloud (qwen2.5, llama3.2)
```

### Step 5: Prepare documents

Place your PIF annual report PDFs in `data/pdfs/` with these naming conventions:
- English: `PIF Annual Report YYYY.pdf` or `PIF-YYYY-Annual-Report-EN.pdf`
- Arabic: `PIF Annual Report YYYY-ar.pdf` or `PIF-YYYY-Annual-Report-AR.pdf`

### Step 6: Process documents (first time only)

```bash
python scripts/process_documents.py
```

This will:
1. Extract text from PDFs (using Docling)
2. Create semantic chunks (using HybridChunker)
3. Generate embeddings (using Ollama qwen3-embedding)
4. Upload to Qdrant vector database

Expected output:
```
INFO: Extraction completed in X.XX seconds
INFO: âœ… Successfully created collection 'PIF_Annual_Report_2023_collection'
INFO: Uploaded batch 1: points 1-100/XXX
INFO: âœ… Successfully processed and verified XXX chunks
```

### Step 7: Run the application

**Terminal 3:**
```bash
streamlit run app.py --server.port=8080
```

Or use the launcher:
```bash
python scripts/run_streamlit.py
```

Access at: **http://localhost:8080**

## ğŸš€ Quick Start (After Initial Setup)

For subsequent runs, you only need:

```bash
# Terminal 1: Start LLM Proxy (if not already running)
python scripts/start_llm_proxy.py

# Terminal 2: Start Streamlit App
streamlit run app.py
```

**Prerequisites must be running:**
- âœ… Ollama service (auto-starts on Windows)
- âœ… Qdrant Docker container (run once with `-d` flag)
- âœ… LLM Proxy (Terminal 1)

## ğŸ–¥ï¸ Usage Guide

### Basic Interaction

1. **Open** http://localhost:8080 in your browser
2. **Introduce yourself**: The bot will ask for your name
3. **Ask questions** about PIF investments, sectors, projects, financials

### Example Questions

**English:**
- "What are PIF's main investment sectors in 2023?"
- "How many jobs did PIF create in 2022?"
- "Tell me about NEOM project funding"
- "What is PIF's sustainability strategy?"

**Arabic:**
- "Ù…Ø§ Ù‡ÙŠ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©ØŸ"
- "ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„ØªÙŠ Ø£Ù†Ø´Ø£Ù‡Ø§ Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚ ÙÙŠ Ù¢Ù Ù¢Ù£ØŸ"

### UI Features

- **ğŸ’¬ Chat Input**: Type questions or use suggested follow-ups
- **ğŸ”„ New Conversation**: Click â†» to start fresh (keeps your name)
- **ğŸ› Debug Mode**: Click â—‰ to show/hide source information
- **ğŸ’¡ Quick Tips**: Click ? for helpful usage tips
- **â¨¯ Logout**: Complete reset and return to home

## ğŸ”§ Advanced Configuration

### LLM Proxy Settings

Edit `config/llm_proxy_config.yaml` to customize:
- Model selection and priorities
- Rate limits and timeouts
- Fallback chains
- Request parameters

### Embedding Settings

Edit `src/core/config.py` to adjust:
- Embedding model (`EMBED_MODEL_ID`)
- Batch sizes (`EMBED_BATCH_SIZE`)
- Vector dimensions (`EMBED_DIMENSION`)
- Chunking parameters (`MAX_TOKENS`)

## ğŸ§¹ Migration & Cleanup

If migrating from old structure:

```bash
# 1. Ensure new structure is working
streamlit run app.py

# 2. Run cleanup script
python scripts/cleanup_old_structure.py

# 3. Confirm deletion when prompted
```

The script will safely remove:
- `api_code/` folder â†’ moved to `src/core/`, `src/retrieval/`, `src/llm/`
- `ui_streamlit/` folder â†’ moved to `src/ui/`
- Old root-level configs â†’ moved to `config/`

## ğŸ” Troubleshooting

### LLM Proxy Not Running
```bash
# Check if proxy is running
curl http://localhost:4000/health

# If not running, start it:
python scripts/start_llm_proxy.py
```

### Ollama Connection Issues
```bash
# Verify Ollama is running
curl http://localhost:11434/api/version

# If not running (Mac/Linux):
ollama serve
```

### Qdrant Not Available
```bash
# Check Docker containers
docker ps

# Restart Qdrant
docker run -d -p 6333:6333 -p 6334:6334 -v "%cd%\data\qdrant_storage":/qdrant/storage qdrant/qdrant
```

For detailed troubleshooting, see [docs/RUN_GUIDE.md](docs/RUN_GUIDE.md)

## ğŸ—ï¸ Tech Stack

### Frontend
- **Streamlit** - Modern Python web framework
- **Custom CSS** - PIF-themed styling (green, gold, black)

### Backend
- **Qdrant** - High-performance vector database
- **Ollama** - Local embeddings (qwen3-embedding, 4096-dim)
- **LiteLLM** - Multi-provider LLM routing
- **Docling** - PDF extraction and processing

### LLM Providers
- **Groq** - Primary (llama-3.1-8b-instant) - FREE & FAST
- **Ollama Cloud** - Fallback (qwen2.5:3b, llama3.2:3b)

## ğŸ“Š Performance Metrics

- **Query Response Time**: ~1-2 seconds (including LLM generation)
- **Embedding Throughput**: ~20-30 texts/second (local Ollama)
- **Retrieval Precision**: 92%+ relevant document retrieval
- **Multi-Year Coverage**: 3 years of PIF annual reports (2021-2023)

## ğŸ“– Documentation

- **[RUN_GUIDE.md](docs/RUN_GUIDE.md)** - Complete setup, troubleshooting, and usage guide
- **[Config Reference](config/README.md)** - Configuration options explained (TODO)

## ğŸ‘¥ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

## ğŸš€ Future Roadmap

- [ ] **Multi-document Support**: Extend to other document types (press releases, reports)
- [ ] **REST API**: Headless API endpoints for integration
- [ ] **Voice Interface**: Speech recognition and text-to-speech
- [ ] **Enhanced Analytics**: Track usage patterns and question types
- [ ] **Fine-tuned Models**: Domain-specific embedding models for finance/investment
- [ ] **Authentication**: User accounts and personalized history
- [ ] **Export Features**: Save conversations as PDF/Markdown
- [ ] **Real-time Data**: Integrate live financial data sources
- [ ] **More Languages**: Expand beyond English and Arabic

## ğŸ“ Support & Contact

- **Issues**: https://github.com/m-monirr/PIF-Annual-Report_RagProject/issues
- **Discussions**: https://github.com/m-monirr/PIF-Annual-Report_RagProject/discussions

## ğŸ“ License

MIT License - See LICENSE file for details

---

**Made with â¤ï¸ for exploring PIF's transformative investments in Saudi Arabia's future**
