# =============================
# LiteLLM Config for RAG
# Groq (Primary) + Ollama Cloud (Fallback)
# =============================

router_settings:
  enable_pre_call_checks: false

litellm_settings:
  # Number of retries handled at LiteLLM-level
  num_retries: 1
  request_timeout: 30
  telemetry: false
  drop_params: true

  # Fallback routing
  fallbacks:
    - rag-llm:  # Primary
      - rag-llm-ollama-1
      - rag-llm-ollama-2
      - rag-llm-groq-fallback

model_list:
  # ---------------------------------------
  # PRIMARY MODEL — Groq llama3.1-8b-instant
  # ---------------------------------------
  - model_name: rag-llm
    litellm_params:
      model: groq/llama-3.1-8b-instant
      api_key: "os.environ/GROQ_API_KEY"
      timeout: 20
      rpm: 30  # Groq is fast, safe limit

  # ---------------------------------------
  # FALLBACK #1 — Ollama Cloud Qwen
  # ---------------------------------------
  - model_name: rag-llm-ollama-1
    litellm_params:
      model: ollama_chat/qwen2.5:latest
      api_base: https://cloud.ollama.ai
      timeout: 30
      rpm: 60
      # For Ollama Cloud: DO NOT add API key unless needed
      # api_key: "os.environ/OLLAMA_API_KEY"

  # ---------------------------------------
  # FALLBACK #2 — Ollama Cloud Llama3.2
  # ---------------------------------------
  - model_name: rag-llm-ollama-2
    litellm_params:
      model: ollama_chat/llama3.2:latest
      api_base: https://cloud.ollama.ai
      timeout: 30
      rpm: 60

  # ---------------------------------------
  # FALLBACK #3 — Groq Mixtral
  # ---------------------------------------
  - model_name: rag-llm-groq-fallback
    litellm_params:
      model: groq/mixtral-8x7b-32768
      api_key: "os.environ/GROQ_API_KEY"
      timeout: 20
      rpm: 30